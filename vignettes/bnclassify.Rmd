---
title: "bnclassify"
author: "Bojan Mihaljevic, Concha Bielza, Pedro Larranaga"
date: "`r Sys.Date()`"
output: 
  rmarkdown::pdf_document:
    toc: true
fontsize: 11pt
vignette: >
  %\VignetteIndexEntry{bnclassify}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

## Intro
Best for complete data but works for incomplete, too. Only discrete.

## Learning algorithms and functions

* Naive Bayes 
    * Attribute-weighted naive Bayes

* Chow-Liu based:
    * Tree augmented naive Bayes
    * Selective tree augmented naive Bayes
  
You can maximize either likelihood, BIC or AIC scores.   

* Wrapper: 
    * Hill-climbing TAN
    * Hill-climbing Super-parent TAN
    * FFSJ
    * BSEJ

By maximizing accuracy.

## Naive Bayes
```{r}
library(bnclassify)
data(car)
nb('class', car)
```

## Chow-Liu based 
<!-- Quadratic time fast. -->
<!-- You can learn with these functions ... -->

```{r}
tan <- tan_bnc(class = 'class', dataset = car)
tan_bic <- tan_bnc(class = 'class', dataset = car, score = 'bic')
plot(tan)
plot(tan_bic)
```

##Wrappers

Using cross-validation, we can use wrappers with the desired number of folds. 

```{r}
a <- tanhc('class', car, k = 10, epsilon = 0, smooth = 1)
b <- tanhc_sp('class', car, k = 10, epsilon = 0, smooth = 1)
c <- bsej('class', car, k = 10, epsilon = 0, smooth = 1)
d <- fssj('class', car, k = 10, epsilon = 0, smooth = 1)
plot(a)
plot(b)
plot(c)
plot(d)
```




## Learning parameters
You call the lp function.

  
bnc('class', car, tan) is equivalent to tan() lp(x, ..)
Note tan is not a string but function object (must have bnclassify attached)


You can use awnb. 

```{r}
nb <- nb('class', car)
anb <- lpawnb(nb, car, smooth = 1, trees = 5, bootstrap_size = 0.5)
```

It is intented for naive Bayes but you can also use it with other classifiers
```{r}
t <- tan_bnc('class', car)
anb <- lpawnb(t, car, smooth = 1, trees = 5, bootstrap_size = 0.5)
```


You can also use the convenience `bnc()` function to do structure and parameter learning in a single call. 

```{r}
a <- bnc('tan_bnc', 'class', car, smooth = 1)
b <- lp(tan_bnc('class', car), car, smooth = 1)
identical(a, b)
```

`bnc()` does not allow AWNB weighted parameters.

## Predicting 

### 0 probabilities
If for some instance there is 0 probability for each class, then a uniform distribution over the classes is returned (not the class prior). 

```{r}
nb <- nb('class', car)
nb <- lp(nb, car[c(1, 700), ], smooth = 0)
predict(object = nb, newdata = car[1000:1001, ], prob = TRUE)
```

### Speed 
It is much faster than gRain and identical to bnlearn.

```{r}
library(microbenchmark)
nb <- nb('class', car)
nb <- lp(nb, car, smooth = 0)
gr <- to_grain(nb)
microbenchmark(predict(object = nb, newdata = car, prob = TRUE))
microbenchmark(gRain::predict.grain(gr, 'class', newdata = car),
                               times = 1)
```

Note that when predicting on a data set with incomplete cases, gRain is used underneath and it will be slow


## Cross-validation
There is rather fast cross-validation. It takes advantage of the modularity of Bayesian networks by accessing only onces the CPTs shared accross structures. In this example it is faster than CV multiple structures.

```{r}
a <- lp(nb('class', car), car, smooth = 1)	
b <- lp(nb('class', car[, 'class', drop = FALSE]), car, smooth = 1)
d <- lp(nb('class', car[, c(sample(1:6, 4), 7), drop = FALSE]), car, smooth = 1)	
set.seed(0)
microbenchmark(r <- cv(list(a, b, d), car, k = 10, dag = FALSE, smooth = 1))
r
```

Keogh has a similar optimization but not quite the same and only for one feature. This is generalized.


You can also cross-validate the awnb computation of the weights. The computation is performed on each fold. 

```{r}
data(voting)
a <- nb('Class', voting)
b <- lpawnb(a, voting, smooth = 1, trees = 40, bootstrap_size = 0.5)
c <- lp(a, voting, smooth = 1)
r <- cv(list(b, c), voting, k = 2, dag = FALSE)
r
```

## Connection to other packages 

```{r}
a <- nb('class', car)
#to_graphNEL()
```

## Feature selection

Some algorithms perform implicit feature selection. E.g., ... For more, use the mlr or feature selector package. See below. 

## mlr package
It's easy to use bnclassify with the mlr package. If you have mlr installed, you just need to call `as_mlr()` to use mlr functions: select features, resample, etc.

```{r}
  library(mlr)
  ctrl = makeFeatSelControlSequential(alpha = 0, method = "sfs")
  rdesc = makeResampleDesc(method = "Holdout")
  ct <- mlr::makeClassifTask(id = "compare", data = car, target = 'class', 
                            fixup.data = "no", check.data = FALSE)  
  nf <- lp(nb('class', car), car, 1)
  bnl <- as_mlr(nf, dag = TRUE)
  sfeats = selectFeatures(learner = bnl, task = ct, resampling = rdesc,
                          control = ctrl, show.info = FALSE)
  sfeats$x
  detach('package:mlr')
```

## Incomplete data
You can learn from incomplete data (just ignoring the missing), but prediction with missing is very slow. This is mainly a problem if you are using wrappers for learning, which is not  advised with incomplete data unless you have little data or a lot of time. Put example microbenchmark with the voting data set...